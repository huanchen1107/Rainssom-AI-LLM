# app.py

import streamlit as st
import json
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage

# å¾ alias_utils åŒ¯å…¥æˆ‘å€‘çš„æ¨™æº–åŒ–å‡½å¼
from alias_utils import normalize_query

# ------------------------------------------------------------------
# 1. å°‡æ‰€æœ‰è€—æ™‚çš„å‰ç½®ä½œæ¥­æ‰“åŒ…æˆä¸€å€‹å‡½å¼ï¼Œä¸¦ç”¨ cache_resource å¿«å–
# ------------------------------------------------------------------
@st.cache_resource
def load_rag_pipeline():
    """
    é€™å€‹å‡½å¼æœƒåŸ·è¡Œæ‰€æœ‰è€—æ™‚çš„åˆå§‹åŒ–æ“ä½œï¼Œä¸¦ä¸” Streamlit æœƒå°‡çµæœå¿«å–èµ·ä¾†ã€‚
    """
    print("--- åŸ·è¡Œè€—æ™‚çš„å‰ç½®ä½œæ¥­ ---")
    
    # --- è¨­å®šæ‚¨çš„ Ollama ä¼ºæœå™¨ä½å€ ---
    # --- è«‹å°‡ IP ä½å€æ›æˆæ‚¨ä¼ºæœå™¨é›»è…¦çš„å€åŸŸç¶²è·¯ IP ---
    OLLAMA_SERVER_URL = "http://192.168.50.56:11434" 


    # 1.1 è¼‰å…¥ä¸¦è™•ç†çŸ¥è­˜åº« (å¾ chunks_raw.json)
    embeddings = OllamaEmbeddings(base_url=OLLAMA_SERVER_URL, model="nomic-embed-text")
    
    try:
        with open("./chunks_raw.json", "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError("æ‰¾ä¸åˆ° 'chunks_raw.json'ã€‚è«‹ç¢ºä¿æª”æ¡ˆèˆ‡ app.py åœ¨åŒä¸€å€‹è³‡æ–™å¤¾ä¸­ã€‚")

    # å°‡ JSON è³‡æ–™è½‰æ›ç‚º LangChain çš„ Document æ ¼å¼
    docs = []
    for item in data:
        # å°‡ 'text' ä½œç‚ºä¸»è¦å…§å®¹ï¼Œå…¶ä»–è³‡è¨Šä½œç‚º metadata
        doc = Document(page_content=item.get('text', ''), metadata={
            'url': item.get('url', ''),
            'title': item.get('title', ''),
            'category': item.get('category', '')
        })
        docs.append(doc)

    if not docs:
        raise ValueError("å¾ 'chunks_raw.json' è¼‰å…¥çš„çŸ¥è­˜åº«æ–‡ä»¶ç‚ºç©ºï¼")

    # 1.2 å»ºç«‹å‘é‡è³‡æ–™åº« (ç›´æ¥ä½¿ç”¨è¼‰å…¥çš„ docs)
    vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    # 1.3 å®šç¾©å„ç¨®è™•ç†éˆ (Chains)
    # -- æŸ¥è©¢é‡å¯«éˆ --
    contextualize_q_system_prompt = """Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages([("system", contextualize_q_system_prompt), ("human", "{chat_history}\n\nFollow Up Input: {question}")])
    contextualize_q_chain = contextualize_q_prompt | Ollama(base_url=OLLAMA_SERVER_URL,model="llama3:8b", temperature=0.1) | StrOutputParser()
    
    # -- å•ç­”éˆ (æ›´æ–°å¾Œçš„ Prompt) --
    qa_system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é†«ç¾è«®è©¢åŠ©ç†ï¼Œä½ çš„åå­—æ˜¯ã€ŒRainssom AIã€ã€‚è«‹æ ¹æ“šæˆ‘æä¾›çš„ã€Œåƒè€ƒè³‡æ–™ã€ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œå°±èªªä½ ä¸çŸ¥é“ã€‚è«‹ä½¿ç”¨å°ç£äººç¿’æ…£çš„ç¹é«”ä¸­æ–‡ä¾†å›ç­”ï¼Œä¸¦ç›¡é‡ä¿æŒç­”æ¡ˆç°¡æ½”ã€‚
    ---
    åƒè€ƒè³‡æ–™:
    {context}
    ---
    """
    qa_prompt = ChatPromptTemplate.from_messages([("system", qa_system_prompt), ("human", "{question}")])
    qa_chain = qa_prompt | Ollama(base_url=OLLAMA_SERVER_URL,model="llama3:8b", temperature=0.1) | StrOutputParser()

    print("--- å‰ç½®ä½œæ¥­å®Œæˆ ---")
    
    return retriever, contextualize_q_chain, qa_chain

# ------------------------------------------------------------------
# 2. Streamlit æ‡‰ç”¨ç¨‹å¼ä¸»é«”
# ------------------------------------------------------------------

# è¨­å®šé é¢æ¨™é¡Œ
st.title("ğŸš€ Rainssom é†«ç¾æ™ºèƒ½åŠ©ç†")

# å‘¼å«å¿«å–å‡½å¼ä¾†è¼‰å…¥ RAG pipeline
try:
    retriever, contextualize_q_chain, qa_chain = load_rag_pipeline()
except Exception as e:
    st.error(f"åˆå§‹åŒ– RAG æµç¨‹å¤±æ•—: {e}")
    st.stop()


# åˆå§‹åŒ– session_state ä¸­çš„å°è©±æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="æ‚¨å¥½ï¼æˆ‘æ˜¯ Rainssom AIï¼Œè«‹å•æœ‰ä»€éº¼å¯ä»¥ç‚ºæ‚¨æœå‹™çš„å—ï¼Ÿ")]

# é¡¯ç¤ºæ­·å²å°è©±è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.markdown(message.content)

# æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input("è«‹åœ¨é€™è£¡è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # å°‡ä½¿ç”¨è€…è¨Šæ¯å­˜å…¥ session_state ä¸¦é¡¯ç¤ºåœ¨ç•«é¢ä¸Š
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human"):
        st.markdown(prompt)

    # æº–å‚™é¡¯ç¤º AI å›æ‡‰
    with st.chat_message("ai"):
        # ä½¿ç”¨ spinner è®“ä½¿ç”¨è€…çŸ¥é“ AI æ­£åœ¨æ€è€ƒ
        with st.spinner("æ€è€ƒä¸­..."):
            # 1. æŸ¥è©¢é‡å¯« (è™•ç†å°è©±æ­·å²)
            rewritten_question = contextualize_q_chain.invoke({
                "chat_history": [msg.content for msg in st.session_state.messages[:-1]],
                "question": prompt
            })

            # 2. **æ–°å¢**ï¼šæ¨™æº–åŒ–æŸ¥è©¢ä¸­çš„é—œéµå­—
            normalized_question = normalize_query(rewritten_question)

            # 3. æª¢ç´¢ (ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„å•é¡Œ)
            retrieved_docs = retriever.invoke(normalized_question)
            context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

            # 4. ç”Ÿæˆç­”æ¡ˆ
            answer = qa_chain.invoke({
                "context": context_text,
                "question": rewritten_question # ä½¿ç”¨é‡å¯«å¾Œã€ä½†æœªæ¨™æº–åŒ–çš„å•é¡Œï¼Œè®“LLMçœ‹åˆ°æœ€åŸå§‹çš„æ„åœ–
            })
            
            # åœ¨ç•«é¢ä¸Šé¡¯ç¤º AI å›æ‡‰
            st.markdown(answer)

    # å°‡ AI å›æ‡‰ä¹Ÿå­˜å…¥ session_state
    st.session_state.messages.append(AIMessage(content=answer))